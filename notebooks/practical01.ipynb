{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and basic NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install necessary libraries\n",
    "!pip install nltk spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary datasets\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Sample text\n",
    "txt = \"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence. It helps computers understand human language. Google, Amazon, and Apple use NLP in their products.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Tokenization\n",
    "print(\"\\nTokenization:\")\n",
    "print(\"Word Tokenization:\", word_tokenize(txt))\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(txt)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"\\nAfter Stopwords Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Stemming:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Lemmatization:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part-of-Speech (POS) Tagging\n",
    "pos_tags = pos_tag(word_tokenize(txt))\n",
    "print(\"\\nPOS Tagging:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Named Entity Recognition (NER) using NLTK\n",
    "chunked = ne_chunk(pos_tags)\n",
    "print(\"\\nNamed Entities (NLTK):\")\n",
    "for subtree in chunked:\n",
    "    if isinstance(subtree, Tree):\n",
    "        print(\" \".join(word for word, tag in subtree.leaves()), \"->\", subtree.label())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Named Entity Recognition (NER) using spaCy\n",
    "doc = nlp(txt)\n",
    "print(\"\\nNamed Entities (spaCy):\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "### Cleaning Text\n",
    "\n",
    "Text preprocessing involves cleaning and normalizing the text data.\n",
    "\"\"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and URLs\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    text = text.replace('[', '').replace(']', '')\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "sample_text = \"Hello!   This is a sample text. It has  numbers like 123 and special characters like @#$.\"\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nCleaned Text:\")\n",
    "print(clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\"\"\"\n",
    "### Sentiment Analysis\n",
    "\n",
    "TextBlob can be used for sentiment analysis. It returns polarity and subjectivity.\n",
    "\"\"\"\n",
    "\n",
    "blob = TextBlob(\"This was a fantastic experience!\")\n",
    "print(\"Polarity (Sentiment):\", blob.sentiment.polarity)  # 1.0 is positive\n",
    "print(\"Subjectivity:\", blob.sentiment.subjectivity)     # 1.0 is very subjective\n",
    "\n",
    "text2 = \"I am disappointed with the service.\"\n",
    "blob2 = TextBlob(text2)\n",
    "print(\"\\nPolarity:\", blob2.sentiment.polarity)\n",
    "print(\"Subjectivity:\", blob2.sentiment.subjectivity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample text file\n",
    "with open(\"sample.txt\", \"w\") as file:\n",
    "    file.write(\"Natural Language Processing (NLP) is a fascinating field of AI.\\n\")\n",
    "    file.write(\"It enables computers to understand, interpret, and generate human language.\\n\")\n",
    "    file.write(\"This text file serves as an example for NLP processing.\\n\")\n",
    "\n",
    "print(\"input.txt has been created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Reading Lines from a Text File\n",
    "\n",
    "We can read a text file line by line using the `readlines()` method.\n",
    "\"\"\"\n",
    "\n",
    "with open('sample.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "print(\"Lines in the file:\")\n",
    "for line in lines:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Introduction to CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) files are widely used for storing tabular data. We'll use the `pandas` library for CSV manipulation.\n",
    "\n",
    "First, let's install `pandas` if you haven't already.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading a CSV File\n",
    "\"\"\"\n",
    "### Reading a CSV File\n",
    "\n",
    "Let's read a sample CSV file using `pandas`.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Writing to a CSV File\n",
    "\"\"\"\n",
    "### Writing to a CSV File\n",
    "\n",
    "We can create a new CSV file from a DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Name': ['David', 'Eve', 'Frank'],\n",
    "    'Age': [28, 32, 40],\n",
    "    'City': ['Miami', 'Seattle', 'Boston']\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(data)\n",
    "print(\"\\nNew DataFrame:\")\n",
    "print(new_df)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "new_df.to_csv('output.csv', index=False)\n",
    "print(\"\\nNew CSV file created successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Advanced CSV Operations\n",
    "\n",
    "## Adding Columns to a CSV File\n",
    "\"\"\"\n",
    "### Adding Columns to a CSV File\n",
    "\n",
    "We can add new columns to an existing DataFrame and save the updated data to a CSV file.\n",
    "\"\"\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('output.csv')\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Add a new column\n",
    "df['Employed'] = ['Yes', 'No', 'Yes']\n",
    "print(\"\\nDataFrame after adding a column:\")\n",
    "print(df)\n",
    "\n",
    "# Write the updated DataFrame to a new CSV file\n",
    "df.to_csv('updated_output.csv', index=False)\n",
    "print(\"\\nUpdated CSV file created successfully.\")\n",
    "\n",
    "## Filtering Rows\n",
    "\"\"\"\n",
    "### Filtering Rows\n",
    "\n",
    "We can filter rows in a DataFrame based on conditions.\n",
    "\"\"\"\n",
    "\n",
    "# Filter rows where Age > 30\n",
    "filtered_df = df[df['Age'] > 30]\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(filtered_df)\n",
    "\n",
    "## Calculating Totals\n",
    "\"\"\"\n",
    "### Calculating Totals\n",
    "\n",
    "We can perform calculations on the data, such as calculating the total age.\n",
    "\"\"\"\n",
    "\n",
    "total_age = df['Age'].sum()\n",
    "print(\"Total Age:\", total_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "### Welcome! \n",
    "\n",
    "In this notebook, we'll learn how to:\n",
    "1. Read and manipulate text data from CSV files\n",
    "2. Preprocess text data for NLP tasks\n",
    "3. Handle labels (e.g., sentiment, categories)\n",
    "4. Extract insights from text data\n",
    "\n",
    "This is essential for tasks like:\n",
    "- Sentiment analysis\n",
    "- Text classification\n",
    "- Named Entity Recognition (NER)\n",
    "- Topic modeling\n",
    "\n",
    "Let's get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Installing Required Libraries\n",
    "\n",
    "### Install Libraries\n",
    "\n",
    "\n",
    "!pip install pandas nltk scikit-learn\n",
    "!python -m nltk downloadpunkt\n",
    "!python -m nltk download stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Importing Libraries\n",
    "\"\"\"\n",
    "### Import Libraries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 4: Create Sample CSV Data\n",
    "\"\"\"\n",
    "### Create Sample CSV Data\n",
    "\n",
    "We'll create a sample CSV file for our exercises.\n",
    "\"\"\"\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is a sample sentence for NLP.\",\n",
    "        \"We are learning text preprocessing in Python.\",\n",
    "        \"Natural Language Processing is fun and challenging.\",\n",
    "        \"I love pizza! It's my favorite food.\",\n",
    "        \"Machine learning is exciting, but sometimes complicated.\",\n",
    "        \"Weather is nice today, perfect for a walk.\",\n",
    "        \"Python is great for NLP tasks.\",\n",
    "        \"Text data can be noisy and unstructured.\",\n",
    "        \"We need to clean and preprocess the text data.\",\n",
    "        \"NLP is the future of AI and human-computer interaction.\"\n",
    "    ],\n",
    "    'label': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Save to CSV\n",
    "df.to_csv('sample_nlp_data.csv', index=False)\n",
    "\n",
    "print(\"Sample CSV file created successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Reading a CSV File\n",
    "\"\"\"\n",
    "### Read a CSV File\n",
    "\n",
    "We'll read the sample CSV file using `pandas`.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('sample_nlp_data.csv')\n",
    "print(\"Data loaded from CSV file:\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Handling Missing Text Data\n",
    "\"\"\"\n",
    "### Handling Missing Text Data\n",
    "\n",
    "Real-world text data often has missing values. Let's see how to handle them.\n",
    "\"\"\"\n",
    "\n",
    "# Introduce missing values\n",
    "df.loc[[2, 5], 'text'] = np.nan\n",
    "\n",
    "\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df)\n",
    "\n",
    "# Handle missing values by replacing with an empty string\n",
    "df['text'].fillna('', inplace=True)\n",
    "\n",
    "print(\"\\nDataFrame after filling missing values:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Extracting Text and Labels\n",
    "\"\"\"\n",
    "### Extracting Text and Labels\n",
    "\n",
    "In NLP, we often work with text and its corresponding labels (e.g., sentiment, category).\n",
    "\"\"\"\n",
    "\n",
    "# Extract text column\n",
    "texts = df['text'].tolist()\n",
    "print(\"Sample texts:\")\n",
    "print(texts[:3])\n",
    "\n",
    "# Extract labels\n",
    "labels = df['label'].tolist()\n",
    "print(\"\\nSample labels:\")\n",
    "print(labels[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Preprocessing Text Data\n",
    "\"\"\"\n",
    "### Preprocessing Text Data\n",
    "\n",
    "Text preprocessing is an essential step in NLP. We'll perform the following steps:\n",
    "1. Lowercasing\n",
    "2. Removing special characters and punctuation\n",
    "3. Removing stop words\n",
    "4. Stemming or lemmatization\n",
    "\"\"\"\n",
    "\n",
    "# Example text\n",
    "sample_text = \"This is a sample sentence for NLP. Natural Language Processing is fun!\"\n",
    "\n",
    "# Step 1: Lowercasing\n",
    "lower_text = sample_text.lower()\n",
    "print(\"Lowercase text:\", lower_text)\n",
    "\n",
    "# Step 2: Removing punctuation\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', lower_text)\n",
    "print(\"\\nText without punctuation:\", clean_text)\n",
    "\n",
    "# Step 3: Tokenization\n",
    "tokens = word_tokenize(clean_text)\n",
    "print(\"\\nTokens:\", tokens)\n",
    "\n",
    "# Step 4: Removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(\"\\nFiltered tokens:\", filtered_tokens)\n",
    "\n",
    "# Step 5: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"\\nStemmed tokens:\", stemmed_tokens)\n",
    "\n",
    "# Step 6: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"\\nLemmatized tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Preprocessing All Rows in the DataFrame\n",
    "\"\"\"\n",
    "### Preprocessing All Rows in the DataFrame\n",
    "\n",
    "We'll apply the preprocessing steps to the entire text column.\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text == '':\n",
    "        return ''\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'_', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply preprocessing to the entire text column\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessed text data:\")\n",
    "print(df[['text', 'preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Handling Special Characters and URLs\n",
    "\"\"\"\n",
    "### Handling Special Characters and URLs\n",
    "\n",
    "Text data often contains URLs, emojis, or special characters that need to be cleaned.\n",
    "\"\"\"\n",
    "\n",
    "# Example text with special characters and URLs\n",
    "text_with_urls = \"Check out this link: https://example.com. #cool\"\n",
    "\n",
    "# Remove URLs\n",
    "text_without_urls = re.sub(r'http\\S+', '', text_with_urls)\n",
    "\n",
    "# Remove special characters\n",
    "text_clean = re.sub(r'[^\\w\\s]', '', text_without_urls)\n",
    "text_clean = re.sub(r'_', '', text_clean)\n",
    "\n",
    "print(\"Original text:\", text_with_urls)\n",
    "print(\"Cleaned text:\", text_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: TF-IDF Vectorization for Text Features\n",
    "\"\"\"\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "We'll convert text data into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(df['preprocessed_text'])\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"\\nFeature names (words):\")\n",
    "print(feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Sentiment Analysis as a Use Case\n",
    "\"\"\"\n",
    "### Sentiment Analysis\n",
    "\n",
    "We'll use the preprocessed text data to perform basic sentiment analysis using `TextBlob`.\n",
    "\"\"\"\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a new TextBlob object for each text\n",
    "df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "print(\"Sentiment scores:\")\n",
    "print(df[['text', 'sentiment']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Text Classification (Labels)\n",
    "\"\"\"\n",
    "### Text Classification\n",
    "\n",
    "We'll demonstrate how to split the data into train and test sets for classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix,\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 14: Conclusion and Resources\n",
    "\"\"\"\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, you learned:\n",
    "1. How to read and manipulate text data from CSV files\n",
    "2. How to preprocess text data for NLP tasks\n",
    "3. How to handle labels and perform sentiment analysis\n",
    "4. How to convert text data into numerical features (TF-IDF)\n",
    "5. How to prepare data for text classification\n",
    "\n",
    "### Resources for Further Learning\n",
    "1. NLTK Documentation: https://www.nltk.org/\n",
    "2. scikit-learn TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "3. TextBlob Documentation: https://textblob.readthedocs.io/en/dev/\n",
    "4. Sentiment Analysis Tutorial: https://realpython.com/python-textblob/\n",
    "\n",
    "Happy coding!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

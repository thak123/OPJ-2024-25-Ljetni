{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and basic NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/gthakkar/miniconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: spacy in /home/gthakkar/miniconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: click in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (2.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: wrapt in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "### Install necessary libraries\n",
    "!pip install nltk spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/gthakkar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gthakkar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gthakkar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/gthakkar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/gthakkar/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/gthakkar/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary datasets\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Sample text\n",
    "txt = \"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence. It helps computers understand human language. Google, Amazon, and Apple use NLP in their products.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization:\n",
      "Word Tokenization: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', '.', 'It', 'helps', 'computers', 'understand', 'human', 'language', '.', 'Google', ',', 'Amazon', ',', 'and', 'Apple', 'use', 'NLP', 'in', 'their', 'products', '.']\n",
      "Sentence Tokenization: ['Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.', 'It helps computers understand human language.', 'Google, Amazon, and Apple use NLP in their products.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Tokenization\n",
    "print(\"\\nTokenization:\")\n",
    "print(\"Word Tokenization:\", word_tokenize(txt))\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stopwords Removal: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'Artificial', 'Intelligence', '.', 'helps', 'computers', 'understand', 'human', 'language', '.', 'Google', ',', 'Amazon', ',', 'Apple', 'use', 'NLP', 'products', '.']\n"
     ]
    }
   ],
   "source": [
    "### Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(txt)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"\\nAfter Stopwords Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stemming: ['natur', 'languag', 'process', '(', 'nlp', ')', 'fascin', 'field', 'artifici', 'intellig', '.', 'help', 'comput', 'understand', 'human', 'languag', '.', 'googl', ',', 'amazon', ',', 'appl', 'use', 'nlp', 'product', '.']\n"
     ]
    }
   ],
   "source": [
    "### Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Stemming:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Lemmatization: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'Artificial', 'Intelligence', '.', 'help', 'computer', 'understand', 'human', 'language', '.', 'Google', ',', 'Amazon', ',', 'Apple', 'use', 'NLP', 'product', '.']\n"
     ]
    }
   ],
   "source": [
    "### Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Lemmatization:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tagging: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('of', 'IN'), ('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('.', '.'), ('It', 'PRP'), ('helps', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), ('Google', 'NNP'), (',', ','), ('Amazon', 'NNP'), (',', ','), ('and', 'CC'), ('Apple', 'NNP'), ('use', 'VBP'), ('NLP', 'NNP'), ('in', 'IN'), ('their', 'PRP$'), ('products', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "### Part-of-Speech (POS) Tagging\n",
    "pos_tags = pos_tag(word_tokenize(txt))\n",
    "print(\"\\nPOS Tagging:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities (NLTK):\n",
      "NLP -> ORGANIZATION\n",
      "Artificial Intelligence -> ORGANIZATION\n",
      "Google -> PERSON\n",
      "Amazon -> GPE\n",
      "NLP -> ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "### Named Entity Recognition (NER) using NLTK\n",
    "chunked = ne_chunk(pos_tags)\n",
    "print(\"\\nNamed Entities (NLTK):\")\n",
    "for subtree in chunked:\n",
    "    if isinstance(subtree, Tree):\n",
    "        print(\" \".join(word for word, tag in subtree.leaves()), \"->\", subtree.label())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities (spaCy):\n",
      "Natural Language Processing -> ORG\n",
      "NLP -> ORG\n",
      "Artificial Intelligence -> ORG\n",
      "Amazon -> ORG\n",
      "Apple -> ORG\n",
      "NLP -> ORG\n"
     ]
    }
   ],
   "source": [
    "### Named Entity Recognition (NER) using spaCy\n",
    "doc = nlp(txt)\n",
    "print(\"\\nNamed Entities (spaCy):\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Hello!   This is a sample text. It has  numbers like 123 and special characters like @#$.\n",
      "\n",
      "Cleaned Text:\n",
      "hello this is a sample text it has numbers like and special characters like\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "### Cleaning Text\n",
    "\n",
    "Text preprocessing involves cleaning and normalizing the text data.\n",
    "\"\"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and URLs\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    text = text.replace('[', '').replace(']', '')\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "sample_text = \"Hello!   This is a sample text. It has  numbers like 123 and special characters like @#$.\"\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nCleaned Text:\")\n",
    "print(clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/gthakkar/miniconda3/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk>=3.9->textblob) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity (Sentiment): 0.5\n",
      "Subjectivity: 0.9\n",
      "\n",
      "Polarity: -0.75\n",
      "Subjectivity: 0.75\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\"\"\"\n",
    "### Sentiment Analysis\n",
    "\n",
    "TextBlob can be used for sentiment analysis. It returns polarity and subjectivity.\n",
    "\"\"\"\n",
    "\n",
    "blob = TextBlob(\"This was a fantastic experience!\")\n",
    "print(\"Polarity (Sentiment):\", blob.sentiment.polarity)  # 1.0 is positive\n",
    "print(\"Subjectivity:\", blob.sentiment.subjectivity)     # 1.0 is very subjective\n",
    "\n",
    "text2 = \"I am disappointed with the service.\"\n",
    "blob2 = TextBlob(text2)\n",
    "print(\"\\nPolarity:\", blob2.sentiment.polarity)\n",
    "print(\"Subjectivity:\", blob2.sentiment.subjectivity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.txt has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a sample text file\n",
    "with open(\"sample.txt\", \"w\") as file:\n",
    "    file.write(\"Natural Language Processing (NLP) is a fascinating field of AI.\\n\")\n",
    "    file.write(\"It enables computers to understand, interpret, and generate human language.\\n\")\n",
    "    file.write(\"This text file serves as an example for NLP processing.\\n\")\n",
    "\n",
    "print(\"input.txt has been created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines in the file:\n",
      "Natural Language Processing (NLP) is a fascinating field of AI.\n",
      "It enables computers to understand, interpret, and generate human language.\n",
      "This text file serves as an example for NLP processing.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "### Reading Lines from a Text File\n",
    "\n",
    "We can read a text file line by line using the `readlines()` method.\n",
    "\"\"\"\n",
    "\n",
    "with open('sample.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "print(\"Lines in the file:\")\n",
    "for line in lines:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/gthakkar/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "### Introduction to CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) files are widely used for storing tabular data. We'll use the `pandas` library for CSV manipulation.\n",
    "\n",
    "First, let's install `pandas` if you haven't already.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New DataFrame:\n",
      "    Name  Age     City\n",
      "0  David   28    Miami\n",
      "1    Eve   32  Seattle\n",
      "2  Frank   40   Boston\n",
      "\n",
      "New CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Reading a CSV File\n",
    "\"\"\"\n",
    "### Reading a CSV File\n",
    "\n",
    "Let's read a sample CSV file using `pandas`.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Writing to a CSV File\n",
    "\"\"\"\n",
    "### Writing to a CSV File\n",
    "\n",
    "We can create a new CSV file from a DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Name': ['David', 'Eve', 'Frank'],\n",
    "    'Age': [28, 32, 40],\n",
    "    'City': ['Miami', 'Seattle', 'Boston']\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(data)\n",
    "print(\"\\nNew DataFrame:\")\n",
    "print(new_df)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "new_df.to_csv('output.csv', index=False)\n",
    "print(\"\\nNew CSV file created successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    Name  Age     City\n",
      "0  David   28    Miami\n",
      "1    Eve   32  Seattle\n",
      "2  Frank   40   Boston\n",
      "\n",
      "DataFrame after adding a column:\n",
      "    Name  Age     City Employed\n",
      "0  David   28    Miami      Yes\n",
      "1    Eve   32  Seattle       No\n",
      "2  Frank   40   Boston      Yes\n",
      "\n",
      "Updated CSV file created successfully.\n",
      "Filtered DataFrame:\n",
      "    Name  Age     City Employed\n",
      "1    Eve   32  Seattle       No\n",
      "2  Frank   40   Boston      Yes\n",
      "Total Age: 100\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Advanced CSV Operations\n",
    "\n",
    "## Adding Columns to a CSV File\n",
    "\"\"\"\n",
    "### Adding Columns to a CSV File\n",
    "\n",
    "We can add new columns to an existing DataFrame and save the updated data to a CSV file.\n",
    "\"\"\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('output.csv')\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Add a new column\n",
    "df['Employed'] = ['Yes', 'No', 'Yes']\n",
    "print(\"\\nDataFrame after adding a column:\")\n",
    "print(df)\n",
    "\n",
    "# Write the updated DataFrame to a new CSV file\n",
    "df.to_csv('updated_output.csv', index=False)\n",
    "print(\"\\nUpdated CSV file created successfully.\")\n",
    "\n",
    "## Filtering Rows\n",
    "\"\"\"\n",
    "### Filtering Rows\n",
    "\n",
    "We can filter rows in a DataFrame based on conditions.\n",
    "\"\"\"\n",
    "\n",
    "# Filter rows where Age > 30\n",
    "filtered_df = df[df['Age'] > 30]\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(filtered_df)\n",
    "\n",
    "## Calculating Totals\n",
    "\"\"\"\n",
    "### Calculating Totals\n",
    "\n",
    "We can perform calculations on the data, such as calculating the total age.\n",
    "\"\"\"\n",
    "\n",
    "total_age = df['Age'].sum()\n",
    "print(\"Total Age:\", total_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "### Welcome! \n",
    "\n",
    "In this notebook, we'll learn how to:\n",
    "1. Read and manipulate text data from CSV files\n",
    "2. Preprocess text data for NLP tasks\n",
    "3. Handle labels (e.g., sentiment, categories)\n",
    "4. Extract insights from text data\n",
    "\n",
    "This is essential for tasks like:\n",
    "- Sentiment analysis\n",
    "- Text classification\n",
    "- Named Entity Recognition (NER)\n",
    "- Topic modeling\n",
    "\n",
    "Let's get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/gthakkar/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in /home/gthakkar/miniconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /home/gthakkar/miniconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: click in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/gthakkar/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "/home/gthakkar/miniconda3/bin/python: No module named nltk.__main__; 'nltk' is a package and cannot be directly executed\n",
      "/home/gthakkar/miniconda3/bin/python: No module named nltk.__main__; 'nltk' is a package and cannot be directly executed\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Installing Required Libraries\n",
    "\n",
    "### Install Libraries\n",
    "\n",
    "\n",
    "!pip install pandas nltk scikit-learn\n",
    "!python -m nltk downloadpunkt\n",
    "!python -m nltk download stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Importing Libraries\n",
    "\"\"\"\n",
    "### Import Libraries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell 4: Create Sample CSV Data\n",
    "\"\"\"\n",
    "### Create Sample CSV Data\n",
    "\n",
    "We'll create a sample CSV file for our exercises.\n",
    "\"\"\"\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is a sample sentence for NLP.\",\n",
    "        \"We are learning text preprocessing in Python.\",\n",
    "        \"Natural Language Processing is fun and challenging.\",\n",
    "        \"I love pizza! It's my favorite food.\",\n",
    "        \"Machine learning is exciting, but sometimes complicated.\",\n",
    "        \"Weather is nice today, perfect for a walk.\",\n",
    "        \"Python is great for NLP tasks.\",\n",
    "        \"Text data can be noisy and unstructured.\",\n",
    "        \"We need to clean and preprocess the text data.\",\n",
    "        \"NLP is the future of AI and human-computer interaction.\"\n",
    "    ],\n",
    "    'label': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Save to CSV\n",
    "df.to_csv('sample_nlp_data.csv', index=False)\n",
    "\n",
    "print(\"Sample CSV file created successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from CSV file:\n",
      "                                                text  label\n",
      "0                 This is a sample sentence for NLP.      0\n",
      "1      We are learning text preprocessing in Python.      1\n",
      "2  Natural Language Processing is fun and challen...      0\n",
      "3               I love pizza! It's my favorite food.      1\n",
      "4  Machine learning is exciting, but sometimes co...      0\n",
      "5         Weather is nice today, perfect for a walk.      1\n",
      "6                     Python is great for NLP tasks.      0\n",
      "7           Text data can be noisy and unstructured.      1\n",
      "8     We need to clean and preprocess the text data.      0\n",
      "9  NLP is the future of AI and human-computer int...      1\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Reading a CSV File\n",
    "\"\"\"\n",
    "### Read a CSV File\n",
    "\n",
    "We'll read the sample CSV file using `pandas`.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('sample_nlp_data.csv')\n",
    "print(\"Data loaded from CSV file:\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with missing values:\n",
      "                                                text  label\n",
      "0                 This is a sample sentence for NLP.      0\n",
      "1      We are learning text preprocessing in Python.      1\n",
      "2                                                NaN      0\n",
      "3               I love pizza! It's my favorite food.      1\n",
      "4  Machine learning is exciting, but sometimes co...      0\n",
      "5                                                NaN      1\n",
      "6                     Python is great for NLP tasks.      0\n",
      "7           Text data can be noisy and unstructured.      1\n",
      "8     We need to clean and preprocess the text data.      0\n",
      "9  NLP is the future of AI and human-computer int...      1\n",
      "\n",
      "DataFrame after filling missing values:\n",
      "                                                text  label\n",
      "0                 This is a sample sentence for NLP.      0\n",
      "1      We are learning text preprocessing in Python.      1\n",
      "2                                                         0\n",
      "3               I love pizza! It's my favorite food.      1\n",
      "4  Machine learning is exciting, but sometimes co...      0\n",
      "5                                                         1\n",
      "6                     Python is great for NLP tasks.      0\n",
      "7           Text data can be noisy and unstructured.      1\n",
      "8     We need to clean and preprocess the text data.      0\n",
      "9  NLP is the future of AI and human-computer int...      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17958/735993009.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['text'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Handling Missing Text Data\n",
    "\"\"\"\n",
    "### Handling Missing Text Data\n",
    "\n",
    "Real-world text data often has missing values. Let's see how to handle them.\n",
    "\"\"\"\n",
    "\n",
    "# Introduce missing values\n",
    "df.loc[[2, 5], 'text'] = np.nan\n",
    "\n",
    "\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df)\n",
    "\n",
    "# Handle missing values by replacing with an empty string\n",
    "df['text'].fillna('', inplace=True)\n",
    "\n",
    "print(\"\\nDataFrame after filling missing values:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts:\n",
      "['This is a sample sentence for NLP.', 'We are learning text preprocessing in Python.', '']\n",
      "\n",
      "Sample labels:\n",
      "[0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Extracting Text and Labels\n",
    "\"\"\"\n",
    "### Extracting Text and Labels\n",
    "\n",
    "In NLP, we often work with text and its corresponding labels (e.g., sentiment, category).\n",
    "\"\"\"\n",
    "\n",
    "# Extract text column\n",
    "texts = df['text'].tolist()\n",
    "print(\"Sample texts:\")\n",
    "print(texts[:3])\n",
    "\n",
    "# Extract labels\n",
    "labels = df['label'].tolist()\n",
    "print(\"\\nSample labels:\")\n",
    "print(labels[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase text: this is a sample sentence for nlp. natural language processing is fun!\n",
      "\n",
      "Text without punctuation: this is a sample sentence for nlp natural language processing is fun\n",
      "\n",
      "Tokens: ['this', 'is', 'a', 'sample', 'sentence', 'for', 'nlp', 'natural', 'language', 'processing', 'is', 'fun']\n",
      "\n",
      "Filtered tokens: ['sample', 'sentence', 'nlp', 'natural', 'language', 'processing', 'fun']\n",
      "\n",
      "Stemmed tokens: ['sampl', 'sentenc', 'nlp', 'natur', 'languag', 'process', 'fun']\n",
      "\n",
      "Lemmatized tokens: ['sample', 'sentence', 'nlp', 'natural', 'language', 'processing', 'fun']\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Preprocessing Text Data\n",
    "\"\"\"\n",
    "### Preprocessing Text Data\n",
    "\n",
    "Text preprocessing is an essential step in NLP. We'll perform the following steps:\n",
    "1. Lowercasing\n",
    "2. Removing special characters and punctuation\n",
    "3. Removing stop words\n",
    "4. Stemming or lemmatization\n",
    "\"\"\"\n",
    "\n",
    "# Example text\n",
    "sample_text = \"This is a sample sentence for NLP. Natural Language Processing is fun!\"\n",
    "\n",
    "# Step 1: Lowercasing\n",
    "lower_text = sample_text.lower()\n",
    "print(\"Lowercase text:\", lower_text)\n",
    "\n",
    "# Step 2: Removing punctuation\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', lower_text)\n",
    "print(\"\\nText without punctuation:\", clean_text)\n",
    "\n",
    "# Step 3: Tokenization\n",
    "tokens = word_tokenize(clean_text)\n",
    "print(\"\\nTokens:\", tokens)\n",
    "\n",
    "# Step 4: Removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(\"\\nFiltered tokens:\", filtered_tokens)\n",
    "\n",
    "# Step 5: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"\\nStemmed tokens:\", stemmed_tokens)\n",
    "\n",
    "# Step 6: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"\\nLemmatized tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text data:\n",
      "                                                text  \\\n",
      "0                 This is a sample sentence for NLP.   \n",
      "1      We are learning text preprocessing in Python.   \n",
      "2                                                      \n",
      "3               I love pizza! It's my favorite food.   \n",
      "4  Machine learning is exciting, but sometimes co...   \n",
      "\n",
      "                                 preprocessed_text  \n",
      "0                              sample sentence nlp  \n",
      "1               learning text preprocessing python  \n",
      "2                                                   \n",
      "3                         love pizza favorite food  \n",
      "4  machine learning exciting sometimes complicated  \n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Preprocessing All Rows in the DataFrame\n",
    "\"\"\"\n",
    "### Preprocessing All Rows in the DataFrame\n",
    "\n",
    "We'll apply the preprocessing steps to the entire text column.\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text == '':\n",
    "        return ''\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'_', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply preprocessing to the entire text column\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessed text data:\")\n",
    "print(df[['text', 'preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Check out this link: https://example.com. #cool\n",
      "Cleaned text: Check out this link  cool\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Handling Special Characters and URLs\n",
    "\"\"\"\n",
    "### Handling Special Characters and URLs\n",
    "\n",
    "Text data often contains URLs, emojis, or special characters that need to be cleaned.\n",
    "\"\"\"\n",
    "\n",
    "# Example text with special characters and URLs\n",
    "text_with_urls = \"Check out this link: https://example.com. #cool\"\n",
    "\n",
    "# Remove URLs\n",
    "text_without_urls = re.sub(r'http\\S+', '', text_with_urls)\n",
    "\n",
    "# Remove special characters\n",
    "text_clean = re.sub(r'[^\\w\\s]', '', text_without_urls)\n",
    "text_clean = re.sub(r'_', '', text_clean)\n",
    "\n",
    "print(\"Original text:\", text_with_urls)\n",
    "print(\"Cleaned text:\", text_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (10, 27)\n",
      "\n",
      "Feature names (words):\n",
      "['ai' 'clean' 'complicated' 'data' 'exciting' 'favorite' 'food' 'future'\n",
      " 'great' 'humancomputer']\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: TF-IDF Vectorization for Text Features\n",
    "\"\"\"\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "We'll convert text data into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(df['preprocessed_text'])\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"\\nFeature names (words):\")\n",
    "print(feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment scores:\n",
      "                                                text  sentiment\n",
      "0                 This is a sample sentence for NLP.     0.0000\n",
      "1      We are learning text preprocessing in Python.     0.0000\n",
      "2                                                        0.0000\n",
      "3               I love pizza! It's my favorite food.     0.5625\n",
      "4  Machine learning is exciting, but sometimes co...    -0.1000\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Sentiment Analysis as a Use Case\n",
    "\"\"\"\n",
    "### Sentiment Analysis\n",
    "\n",
    "We'll use the preprocessed text data to perform basic sentiment analysis using `TextBlob`.\n",
    "\"\"\"\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a new TextBlob object for each text\n",
    "df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "print(\"Sentiment scores:\")\n",
    "print(df[['text', 'sentiment']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (8, 27)\n",
      "Test set shape: (2, 27)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Text Classification (Labels)\n",
    "\"\"\"\n",
    "### Text Classification\n",
    "\n",
    "We'll demonstrate how to split the data into train and test sets for classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix,\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 14: Conclusion and Resources\n",
    "\"\"\"\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, you learned:\n",
    "1. How to read and manipulate text data from CSV files\n",
    "2. How to preprocess text data for NLP tasks\n",
    "3. How to handle labels and perform sentiment analysis\n",
    "4. How to convert text data into numerical features (TF-IDF)\n",
    "5. How to prepare data for text classification\n",
    "\n",
    "### Resources for Further Learning\n",
    "1. NLTK Documentation: https://www.nltk.org/\n",
    "2. scikit-learn TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "3. TextBlob Documentation: https://textblob.readthedocs.io/en/dev/\n",
    "4. Sentiment Analysis Tutorial: https://realpython.com/python-textblob/\n",
    "\n",
    "Happy coding!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
